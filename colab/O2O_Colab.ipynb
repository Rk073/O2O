{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning a Stable Transition: A Representation-First Approach to Offline-to-Online Reinforcement Learning\n\n",
        "**Abstract**: A core difficulty in Offline-to-Online (O2O) RL is mitigating \"value hallucination\" (as seen in model-based RL) when the agent encounters novel states during online interaction. While replay-based methods implicitly manage this transition, they often lack an explicit mechanism to quantify the distribution shift, leading to instability. We propose a \"representation-first\" O2O framework that decouples the memory of the offline data from the online learning process. We first learn a Data Support Representation (DSR), a density or energy-based model, exclusively on the offline dataset. This DSR provides a continuous measure of support for any state-action pair. During the online phase, this DSR is used in two ways: (1) as a pessimism regularizer for the value function, scaling conservatism inversely with the data support, and (2) as an intrinsic exploration bonus, guiding the agent to efficiently explore regions just beyond the boundary of the offline dataset. This approach removes the need for a shared replay buffer, aligns with work on continual learning representations, and allows the online learner (e.g., a simple on-policy agent) to adapt without catastrophic overestimation. We show that our method prevents the common performance drop seen in O2O fine-tuning and achieves a more stable and monotonic improvement in online performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "setup"},
      "outputs": [],
      "source": [
        "# Setup: install minimal dependencies (Gymnasium Mujoco, Mujoco engine, Torch, NumPy, h5py)\n",
        "!pip -q install gymnasium[mujoco]==0.29.1 mujoco==3.1.6 torch numpy h5py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "mount"},
      "outputs": [],
      "source": [
        "# Mount Google Drive and add repo to PYTHONPATH\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os, sys\n",
        "REPO_ROOT = '/content/drive/MyDrive/O2O'  # change if you store it elsewhere\n",
        "sys.path.append(REPO_ROOT)\n",
        "print('Repo root:', REPO_ROOT)\n",
        "!ls -la \"$REPO_ROOT\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "config"},
      "outputs": [],
      "source": [
        "# Config â€” choose task and directories in Drive\n",
        "TASK = 'hopper-medium-v2'  # or 'walker2d-medium-v2'\n",
        "ENV_MAP = {'hopper-medium-v2':'Hopper-v4','walker2d-medium-v2':'Walker2d-v4'}\n",
        "ENV_ID = ENV_MAP[TASK]\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/O2O'\n",
        "DATA_DIR = f'{DRIVE_ROOT}/data'\n",
        "CKPT_DIR = f'{DRIVE_ROOT}/checkpts'\n",
        "LOG_DIR = f'{DRIVE_ROOT}/logs'\n",
        "for d in (DATA_DIR, CKPT_DIR, LOG_DIR):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "H5_FILE = {'hopper-medium-v2':'hopper-medium-v2.hdf5','walker2d-medium-v2':'walker2d-medium-v2.hdf5'}[TASK]\n",
        "H5_PATH = f'{DATA_DIR}/{H5_FILE}'\n",
        "NPZ_PATH = f\"{DATA_DIR}/{TASK.replace('-v2','').replace('-','_')}.npz\"\n",
        "DSR_OUT = f\"{CKPT_DIR}/{TASK.replace('-v2','').replace('-','_')}_dsr.pt\"\n",
        "# Optional: Behavior Cloning pretraining for policy init\n",
        "RUN_BC = True\n",
        "BC_ACTOR = f\"{CKPT_DIR}/{TASK.replace('-v2','').replace('-','_')}_bc_actor.pt\"\n",
        "print('ENV_ID:', ENV_ID)\n",
        "print('H5:', H5_PATH)\n",
        "print('NPZ:', NPZ_PATH)\n",
        "print('DSR:', DSR_OUT)\n",
        "print('BC actor (if RUN_BC):', BC_ACTOR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "download"},
      "outputs": [],
      "source": [
        "# Download D4RL-style HDF5 and convert to NPZ (states/actions)\n",
        "import urllib.request, h5py, numpy as np\n",
        "BASE_URL = 'https://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2'\n",
        "if not os.path.exists(H5_PATH):\n",
        "    url = f\"{BASE_URL}/{H5_FILE}\"\n",
        "    print('Downloading', url)\n",
        "    urllib.request.urlretrieve(url, H5_PATH)\n",
        "    print('Saved', H5_PATH)\n",
        "else:\n",
        "    print('Found HDF5', H5_PATH)\n",
        "def h5_to_npz(h5_path, npz_path, max_samples=None):\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        obs = np.array(f['observations'], dtype=np.float32)\n",
        "        acts = np.array(f['actions'], dtype=np.float32)\n",
        "    if max_samples is not None and max_samples < len(obs):\n",
        "        idx = np.random.permutation(len(obs))[:max_samples]\n",
        "        obs, acts = obs[idx], acts[idx]\n",
        "    np.savez_compressed(npz_path, states=obs, actions=acts)\n",
        "    return obs.shape, acts.shape\n",
        "if not os.path.exists(NPZ_PATH):\n",
        "    shapes = h5_to_npz(H5_PATH, NPZ_PATH, max_samples=None)\n",
        "    print('Converted to NPZ', NPZ_PATH, 'shapes=', shapes)\n",
        "else:\n",
        "    print('Found NPZ', NPZ_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "train_dsr"},
      "outputs": [],
      "source": [
        "# Train DSR on the offline dataset using the repo script\n",
        "import torch\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', DEVICE)\n",
        "!python {REPO_ROOT + '/train_dsr.py'} --offline_path {NPZ_PATH} --env_id {ENV_ID} --dsr_out {DSR_OUT} --epochs 20 --batch_size 2048 --device {DEVICE}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "pretrain_bc"},
      "outputs": [],
      "source": [
        "# Optional: Behavior Cloning pretraining to warm-start the actor\n",
        "if RUN_BC:\n",
        "    print('Running BC pretraining...')\n",
        "    !python {REPO_ROOT + '/pretrain_bc.py'} --offline_path {NPZ_PATH} --env_id {ENV_ID} --out_actor {BC_ACTOR} --device {DEVICE} --epochs 20 --batch_size 1024\n",
        "else:\n",
        "    print('Skipping BC pretraining (RUN_BC=False)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "init_flag"},
      "outputs": [],
      "source": [
        "# Compose optional init flag for PPO if BC was run\n",
        "INIT_ACTOR_ARG = f'--init_actor {BC_ACTOR}' if RUN_BC else ''\n",
        "print('INIT_ACTOR_ARG:', INIT_ACTOR_ARG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "train_online"},
      "outputs": [],
      "source": [
        "# Run online PPO with DSR pessimism + bonus\n",
        "TOTAL_STEPS = 300_000\n",
        "STEPS_PER_EPOCH = 4096\n",
        "MINIBATCH = 256\n",
        "ITERS = 10\n",
        "!python {REPO_ROOT + '/train_online.py'} --env_id {ENV_ID} --dsr_path {DSR_OUT} --total_steps {TOTAL_STEPS} --steps_per_epoch {STEPS_PER_EPOCH} --minibatch_size {MINIBATCH} --train_iters {ITERS} --device {DEVICE} --log_csv {LOG_DIR} {INIT_ACTOR_ARG}\n"
      ]
    }
  ],
  "metadata": {
    "colab": {"name": "O2O_Colab.ipynb", "provenance": []},
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
