Learning a Stable Transition: Representation-First Offline-to-Online RL

Overview
- This project implements a representation-first Offline-to-Online (O2O) RL framework.
- A Data Support Representation (DSR) is trained only on the offline dataset to measure support for state-action pairs.
- During online learning with PPO, the DSR is used:
  - as a pessimism regularizer on the value function (stronger where support is low)
  - as an intrinsic exploration bonus near the support boundary to guide safe exploration.
- The online learner does not share a replay buffer with the offline data.

Repo Structure
- `o2o/config.py`: Typed configs for DSR and PPO.
- `o2o/datasets.py`: Offline dataset loading and helpers.
- `o2o/models/dsr.py`: DSR model and trainer (energy-style classifier on (s,a)).
- `o2o/models/ppo.py`: PPO agent with actor-critic, integrates DSR pessimism and intrinsic bonus.
- `o2o/utils/support_bonus.py`: Support-to-bonus shaping.
- `o2o/utils/envs.py`: Gym/Gymnasium env helpers and action-space utilities.
- `train_dsr.py`: Train DSR on offline dataset and save.
- `train_online.py`: Run online PPO with pretrained DSR.

Requirements
- Python 3.9+
- PyTorch
- Gymnasium or Gym (either is fine)
- NumPy

Quickstart
1) Prepare offline dataset (NPZ with `states` and `actions`):
   - Use `o2o/datasets.py` utility to collect random rollouts:
     `python -m o2o.datasets --env_id CartPole-v1 --episodes 50 --out data/cartpole_offline.npz`

2) Train DSR on offline data:
   `python train_dsr.py --offline_path data/cartpole_offline.npz --env_id CartPole-v1 --dsr_out checkpts/cartpole_dsr.pt`

3) Online training with PPO + DSR:
   `python train_online.py --env_id CartPole-v1 --dsr_path checkpts/cartpole_dsr.pt --total_steps 200000`

Design Notes
- DSR is a small MLP trained as a binary energy-style classifier: offline (s,a) are positives; negatives are generated by sampling actions from a broad prior (uniform over action space for continuous, uniform categorical for discrete). The sigmoid output is a calibrated support score in [0,1].
- Pessimism regularizer: the critic loss includes `beta * E[(1 - support)^gamma * V(s)^2]`, pushing values down in low-support regions to avoid overestimation.
- Intrinsic bonus: a smooth boundary-focused bonus `b(s,a)` that peaks just beyond the support frontier, encouraging safe, progressive expansion.

Notes
- If you use a continuous-action environment, ensure action bounds are provided by the env; otherwise set them via flags.
- You can extend DSR with better negative sampling or encoders; this implementation is intentionally simple and self-contained.

