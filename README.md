Learning a Stable Transition: Representation-First Offline-to-Online RL

Overview
- This project implements a representation-first Offline-to-Online (O2O) RL framework.
- A Data Support Representation (DSR) is trained only on the offline dataset to measure support for state-action pairs.
- During online learning with PPO, the DSR is used:
  - as a pessimism regularizer on the value function (stronger where support is low)
  - as an intrinsic exploration bonus near the support boundary to guide safe exploration.
- The online learner does not share a replay buffer with the offline data.

Colab
- Clone this repo in a Colab notebook and run `pip install -r requirements.txt`.
- Use the provided notebook `notebooks/colab_o2o.ipynb` for a turnkey CartPole demo.
- The notebook collects a tiny offline dataset, trains a DSR, then fine-tunes online with PPO + pessimism + intrinsic bonus. It logs to CSV and plots curves.

Repo Structure
- `o2o/config.py`: Typed configs for DSR and PPO.
- `o2o/datasets.py`: Offline dataset loading and helpers.
- `o2o/models/dsr.py`: DSR model and trainer (energy-style classifier on (s,a)).
- `o2o/models/ppo.py`: PPO agent with actor-critic, integrates DSR pessimism and intrinsic bonus.
- `o2o/utils/support_bonus.py`: Support-to-bonus shaping.
- `o2o/utils/envs.py`: Gym/Gymnasium env helpers and action-space utilities.
- `train_dsr.py`: Train DSR on offline dataset and save.
- `train_online.py`: Run online PPO with pretrained DSR.

Requirements
- Python 3.9+
- PyTorch
- Gymnasium or Gym (either is fine)
- NumPy

Optional (for plots in the Colab/demo):
- pandas
- matplotlib

Quickstart
1) Prepare offline dataset (NPZ with `states` and `actions`):
   - Use `o2o/datasets.py` utility to collect random rollouts:
     `python -m o2o.datasets --env_id CartPole-v1 --episodes 50 --out data/cartpole_offline.npz`

2) Train DSR on offline data:
   `python train_dsr.py --offline_path data/cartpole_offline.npz --env_id CartPole-v1 --dsr_out checkpts/cartpole_dsr.pt`

3) Online training with PPO + DSR:
   `python train_online.py --env_id CartPole-v1 --dsr_path checkpts/cartpole_dsr.pt --total_steps 200000`
   Useful knobs:
   - Pessimism schedule: `--pess_alpha0 1.0 --pess_alpha_final 0.1 --pess_anneal_steps 1e5 --pess_gamma 1.0`
   - Advantage gating: `--adv_gate_tau 0.5 --adv_gate_k 5.0` (0 disables)
   - Exploration bonus: `--bonus_type boundary|entropy|ucb --bonus_eta 0.1 --bonus_center 0.7 --bonus_sigma 0.15`
   - PPO stability: `--ent_coeff 0.01 --target_kl 0.02 --vf_clip 0.1 --actor_grad_clip 1.0 --critic_grad_clip 1.0`
   - Support-adaptive trust region: `--support_adaptive_clip`
   - Observation norm (PPO only): `--obs_norm --obs_norm_clip 10.0`
   - BC anchor (stability on continuous control): `--init_actor <bc.pt> --kl_bc_coef 0.5 --kl_bc_pow 1.0` or anneal with `--kl_bc_coef0 0.5 --kl_bc_coef_final 0.1 --kl_bc_anneal_steps 1e5`
   - Logging: `--log_csv logs/run.csv`

Example: HalfCheetah-v4 (D4RL medium)
- Behavior Cloning (GPU):
  `python pretrain_bc.py --offline_path data/halfcheetah_medium_v0.npz --env_id HalfCheetah-v4 --out_actor checkpts/halfcheetah_bc_actor.pt --device cuda --epochs 20 --batch_size 1024 --hidden 256 256 --activation tanh --lr 3e-4`
- DSR training (GPU, robust + calibrated):
  `python train_dsr.py --offline_path data/halfcheetah_medium_v0.npz --env_id HalfCheetah-v4 --device cuda --epochs 100 --dsr_out checkpts/halfcheetah_dsr.pt --batch_size 512 --num_negatives 4 --temperature 1.0 --val_split 0.1 --early_stop_patience 5 --neg_modes uniform shuffle jitter --neg_weights 0.5 0.25 0.25 --jitter_std 0.05 --hidden 256 256 --grad_clip 1.0 --calibrate_temperature`
- Online PPO (GPU, stable settings):
  `python train_online.py --env_id HalfCheetah-v4 --dsr_path checkpts/halfcheetah_dsr.pt --init_actor checkpts/halfcheetah_bc_actor.pt --total_steps 200000 --steps_per_epoch 2048 --minibatch_size 256 --train_iters 10 --hidden 256 256 --device cuda --obs_norm --ent_coeff 0.01 --target_kl 0.02 --vf_clip 0.1 --actor_grad_clip 1.0 --critic_grad_clip 1.0 --support_adaptive_clip --bonus_type boundary --bonus_eta 0.02 --bonus_center 0.7 --bonus_sigma 0.15 --pess_alpha0 0.5 --pess_alpha_final 0.2 --pess_anneal_steps 100000 --pess_gamma 2.0 --adv_gate_tau0 0.7 --adv_gate_tau_final 0.5 --adv_gate_tau_anneal_steps 100000 --adv_gate_k 10.0 --kl_bc_coef0 0.5 --kl_bc_coef_final 0.1 --kl_bc_anneal_steps 100000 --kl_bc_pow 2.0 --log_csv logs/cheetah_run.csv`

Design Notes
- DSR is a small MLP trained as a binary energy-style classifier: offline (s,a) are positives; negatives are generated by sampling actions from a broad prior (uniform over action space for continuous, uniform categorical for discrete). The sigmoid output is a calibrated support score in [0,1].
- Pessimism regularizer: the value target is shaped as `R_t - alpha * (1 - support)^gamma`, pushing values down in low-support regions to avoid overestimation. `alpha` is annealed from `pess_alpha0` to `pess_alpha_final` over `pess_anneal_steps`.
- BC Anchor: an optional frozen BC policy provides a support-weighted KL-style regularizer; use fixed `--kl_bc_coef` or a schedule with `--kl_bc_coef0/--kl_bc_coef_final/--kl_bc_anneal_steps`.
- Observation normalization (PPO only): stabilizes high-dimensional control while preserving representation-first separation (DSR uses raw obs).
- Support-adaptive trust region: optionally shrinks PPO clip range in low-support regions for more cautious updates.
- Intrinsic bonus: a smooth boundary-focused bonus `b(s,a)` that peaks just beyond the support frontier, encouraging safe, progressive expansion.

Notes
- If you use a continuous-action environment, ensure action bounds are provided by the env; otherwise set them via flags.
- You can extend DSR with better negative sampling or encoders; this implementation is intentionally simple and self-contained.
